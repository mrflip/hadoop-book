commit 800fcdb9cb14ba5f14dc06c1587febfaa0fbbb22
Author: Philip (flip) Kromer <flip@infochimps.org>
Date:   Mon Jul 26 01:06:56 2010 -0500

    Chopped a bunch of lines from the output files

diff --git a/blog_posts/hadoop_book/flip.xml b/blog_posts/hadoop_book/flip.xml
index 085c21b..8ec0ebb 100644
--- a/blog_posts/hadoop_book/flip.xml
+++ b/blog_posts/hadoop_book/flip.xml
@@ -141,24 +141,11 @@ nealrichter     tlipcon</screen>
         <para>Then find all edges out from each node by grouping on source:</para>
         <programlisting>replies_out = GROUP a_replies_b BY source;
 DUMP replies_out</programlisting>
-        <screen>(CMastication,{(esammer,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(esammer,CMastication),(esammer,CMastication),(ChrisDiehl,CMastication),(esammer,CMastication),(peteskomoroch,CMastication),(esammer,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(esammer,CMastication),(bradfordcross,CMastication),(peteskomoroch,CMastication)})
-(ChrisDiehl,{(DataJunkie,ChrisDiehl),(DataJunkie,ChrisDiehl),(peteskomoroch,ChrisDiehl)})
-(DataJunkie,{(CMastication,DataJunkie),(CMastication,DataJunkie),(nealrichter,DataJunkie),(peteskomoroch,DataJunkie),(mat_kelcey,DataJunkie),(CMastication,DataJunkie),(communicating,DataJunkie),(ChrisDiehl,DataJunkie),(bradfordcross,DataJunkie),(nealrichter,DataJunkie),(mat_kelcey,DataJunkie),(mat_kelcey,DataJunkie)})
-(LusciousPear,{(bradfordcross,LusciousPear),(metcalfc,LusciousPear),(mikeolson,LusciousPear),(metcalfc,LusciousPear),(metcalfc,LusciousPear),(tlipcon,LusciousPear),(kevinweil,LusciousPear),(bradfordcross,LusciousPear),(bradfordcross,LusciousPear),(DataJunkie,LusciousPear),(esammer,LusciousPear)})
-(bradfordcross,{(LusciousPear,bradfordcross),(CMastication,bradfordcross),(mrflip,bradfordcross),(LusciousPear,bradfordcross),(peteskomoroch,bradfordcross),(ogrisel,bradfordcross),(LusciousPear,bradfordcross),(DataJunkie,bradfordcross),(ogrisel,bradfordcross)})
-(communicating,{(nealrichter,communicating),(DataJunkie,communicating),(ogrisel,communicating),(ogrisel,communicating)})
-(cutting,{(tom_e_white,cutting)})
-(esammer,{(CMastication,esammer),(CMastication,esammer),(mikeolson,esammer),(mikeolson,esammer),(jeromatron,esammer)})
-(jeromatron,{(mrflip,jeromatron),(mrflip,jeromatron)})
+        <screen>(cutting,{(tom_e_white,cutting)})
 (josephkelly,{(wattsteve,josephkelly)})
-(kevinweil,{(tlipcon,kevinweil),(DataJunkie,kevinweil),(mikeolson,kevinweil),(LusciousPear,kevinweil),(tlipcon,kevinweil),(tlipcon,kevinweil),(DataJunkie,kevinweil),(mrflip,kevinweil),(LusciousPear,kevinweil)})
-(metcalfc,{(nealrichter,metcalfc),(nealrichter,metcalfc),(nealrichter,metcalfc)})
 (mikeolson,{(LusciousPear,mikeolson),(kevinweil,mikeolson),(LusciousPear,mikeolson),(tlipcon,mikeolson)})
 (mndoci,{(mrflip,mndoci),(peteskomoroch,mndoci),(LusciousPear,mndoci),(mrflip,mndoci)})
 (mrflip,{(LusciousPear,mrflip),(mndoci,mrflip),(mndoci,mrflip),(ogrisel,mrflip),(esammer,mrflip),(ogrisel,mrflip),(esammer,mrflip),(esammer,mrflip),(wattsteve,mrflip)})
-(mza,{(mrflip,mza)})
-(nealrichter,{(metcalfc,nealrichter),(DataJunkie,nealrichter),(peteskomoroch,nealrichter),(metcalfc,nealrichter)})
-(ogrisel,{(mrflip,ogrisel),(bradfordcross,ogrisel),(mrflip,ogrisel)})
 (peteskomoroch,{(CMastication,peteskomoroch),(esammer,peteskomoroch),(DataJunkie,peteskomoroch),(CMastication,peteskomoroch),(mndoci,peteskomoroch),(CMastication,peteskomoroch),(nealrichter,peteskomoroch),(nealrichter,peteskomoroch),(bradfordcross,peteskomoroch),(ChrisDiehl,peteskomoroch),(mrflip,peteskomoroch),(mrflip,peteskomoroch),(LusciousPear,peteskomoroch),(bradfordcross,peteskomoroch),(ChrisDiehl,peteskomoroch),(nealrichter,peteskomoroch),(CMastication,peteskomoroch),(mndoci,peteskomoroch)})
 (tlipcon,{(mrflip,tlipcon),(LusciousPear,tlipcon),(LusciousPear,tlipcon),(nealrichter,tlipcon),(LusciousPear,tlipcon),(nealrichter,tlipcon),(mrflip,tlipcon),(kevinweil,tlipcon)})
 (tom_e_white,{(mrflip,tom_e_white),(lenbust,tom_e_white)})</screen>
@@ -177,27 +164,14 @@ replies_in_degree = FOREACH replies_in {
   GENERATE group, COUNT(nbrs), COUNT(a_replies_b);
 };
 DUMP replies_in_degree</programlisting>
-        <screen>(mza,1L,1L)
+        <screen>(cutting,1L,1L)
+(josephkelly,1L,1L)
+(mikeolson,3L,4L)
 (mndoci,3L,4L)
 (mrflip,5L,9L)
-(cutting,1L,1L)
-(esammer,3L,5L)
-(ogrisel,2L,3L)
+(peteskomoroch,9L,18L)
 (tlipcon,4L,8L)
-(metcalfc,1L,3L)
-(kevinweil,5L,9L)
-(mikeolson,3L,4L)
-(ChrisDiehl,2L,3L)
-(DataJunkie,7L,12L)
-(jeromatron,1L,2L)
-(josephkelly,1L,1L)
-(nealrichter,3L,4L)
-(tom_e_white,2L,2L)
-(CMastication,4L,16L)
-(LusciousPear,7L,11L)
-(bradfordcross,6L,9L)
-(communicating,3L,4L)
-(peteskomoroch,9L,18L)</screen>
+(tom_e_white,2L,2L)</screen>
         <para>In this sample <literal>@mrflip</literal>, for example, has five neighbors and 9 incoming replies.</para>
         <para>In Twitter, the in-degree can vary wildly. Most nodes have very few edges, but a few celebrities—such as <literal>@THE_REAL_SHAQ</literal> (basketball star Shaquille O’Neill) or <literal>@sockington</literal> (a fictional cat)—have millions. By contrast, almost every intersection on a road map is 4-way.<footnote>
             <para>The largest outlier that comes to mind is the famous "Magic Roundabout" in Swindon, England, with degree 10, <ulink

commit 59d0e95734a0c12d5e255ca32d3918d5d6150900
Author: Philip (flip) Kromer <flip@infochimps.org>
Date:   Mon Jul 26 00:47:09 2010 -0500

    Edits to prose and flow, feedback from TW

diff --git a/blog_posts/hadoop_book/flip.xml b/blog_posts/hadoop_book/flip.xml
index ec3ddd0..085c21b 100644
--- a/blog_posts/hadoop_book/flip.xml
+++ b/blog_posts/hadoop_book/flip.xml
@@ -8,17 +8,17 @@
     <para>Using Pig and Wukong to Explore Billion-edge Network Graphs</para>
     <sect2>
       <title>Basics</title>
-      <para>Networks at massive scale are fascinating. The number of things they model are extremely general: if you have a collection of things (that we'll call nodes), they are related (edges), and the nodes and edges tell a story (node / edge metadata), you have a network graph.</para>
+      <para>Networks at massive scale are fascinating. The number of things they model are extremely general: if you have a collection of things (that we’ll call nodes), they are related (edges), and the nodes and edges tell a story (node / edge metadata), you have a network graph.</para>
       <para>I started the Infochimps project, a site to find, share or sell ay dataset in the world. At Infochimps, we’ve got a whole bag of tricks ready to apply to any interesting network graph that comes into the collection. We chiefly use Pig (described in chapter XXXX) and Wukong (<ulink
           url="http://github.com/mrflip/wukong"
-        />), a toolkit we've developed for Hadoop streaming in the Ruby programming language. They let us write simple scripts like the ones below—almost all of which fit on a single printed page—to process networks as large as 100 million nodes and 2 billion edges.  Here are a few datasets that come up in a search for “network” on infochimps.org<footnote>
+        />), a toolkit we’ve developed for Hadoop streaming in the Ruby programming language. They let us write simple scripts like the ones below—almost all of which fit on a single printed page—to process networks as large as 100 million nodes and 2 billion edges.  Here are a few datasets that come up in a search for “network” on infochimps.org<footnote>
               <para><ulink
                   url="http://infochimps.org/search?query=network"
                 /></para>
             </footnote>:</para>
       <itemizedlist>
         <listitem>
-          <para>A social network, such as Twitter or Facebook.  We somewhat impersonally model people as nodes, and relationships (<literal>@mrflip</literal> is friends with <literal>@tom_e_white</literal>) or actions (<literal>@infochimps</literal> replied to <literal>@hadoop</literal>) as edges. The number of messages a user has sent and the bag of words from all those messages are each important pieces of node metadata.</para>
+          <para>A social network, such as Twitter or Facebook.  We somewhat impersonally model people as nodes, and relationships (<literal>@mrflip</literal> is friends with <literal>@tom_e_white</literal>) or actions (<literal>@infochimps</literal> mentioned <literal>@hadoop</literal>) as edges. The number of messages a user has sent and the bag of words from all those messages are each important pieces of node metadata.</para>
         </listitem>
         <listitem>
           <para>A linked document collection such as Wikipedia or the entire web. Each page is a node (carrying its title, view count and categories as node metadata). Each hyperlink is an edge, and the frequency at which people click from one page to the next is edge metadata.</para>
@@ -36,7 +36,7 @@
               <para><ulink url="http://www.openstreetmap.org/"/></para>
             </footnote></para>
         </listitem>
-      <listitem>Or more esoteric graphs that fall out when you take aninteresting problem and shake it just right. Take a few million Twitter messagesand emit an edge for every pair of non-keyboard characters within the samemessage. Simply by observing “often, when humans use 最, they also use 近” youcan recreate a map of human languages (see <xref
+      <listitem>Or the many esoteric graphs that fall out if you take an interesting system and shake it just right. Stream through a few million Twitter messages, and emit an edge for every pair of non-keyboard characters ocurring within the same message. Simply by observing “often, when humans use 最, they also use 近” youcan recreate a map of human languages (see <xref
           linkend="TwitterLanguageMap"/>).</listitem>
       </itemizedlist>
       <figure id="TwitterLanguageMap">
@@ -100,19 +100,23 @@
     not reply_tweet_id.blank?
   true
 end</programlisting>
-      <para>Anyone can pull gigabytes of data per day from Twitter’s streaming API: follow the instructions at <ulink
-          url="http://dev.twitter.com/doc/get/statuses/sample"/> or use a tool like <ulink
-          url="http://github.com/hayesdavis/flamingo"/>.</para>
+      <para>Anyone can pull gigabytes of data per day from Twitter’s streaming API: follow the instructions on Twitter’s developer site (<ulink
+          url="http://dev.twitter.com/doc/get/statuses/sample"/>) or use a tool like Hayes Davis’ Flamingo (<ulink
+          url="http://github.com/hayesdavis/flamingo"/>).</para>
       <para>Twitter messages typically arrive in raw JSON format. Here are a few tweets:</para>
-      <literallayout><literal>{"id":3239897342,"screen_name":"tom_e_white","text":"Just finished the final draft for Hadoop: the Definitive Guide!","reply_screen_name":null,"reply_tweet_id":null,...}
-{"id":3239873453,"screen_name":"mrflip","text":"@tom_e_white Can't wait to get a copy!","reply_screen_name":"tom_e_white","reply_tweet_id":3239897342,...}
-{"id":3235751418,"screen_name":"drsm79","text":"Lots of ideas buzzing round my head but not quite crystalising, so can't sleep. Want to read Tom's hadoop book, too - thanks @tom_e_white!!","reply_screen_name":null,"reply_tweet_id":null,...}
-{"id":16434069252,"screen_name":"wattsteve","text":"@josephkelly great job on the #InfoChimps API. Next time I come by remind me to tell you about the time a baboon broke into our house.","reply_screen_name":"josephkelly",...}
-{"id":7809927173,"screen_name":"mrflip","text":"@mza Re: http://bit.ly/atbroxmr Have you seen @James_Rubino's http://bit.ly/clusterfork ? Lots of good hadoop refs there too","reply_screen_name":"@mza",...}
-{"id":4491069515,"screen_name":"nealrichter","text":"@tlipcon divide lots of data into little parts.
-Magic software gnomes fix up the parts, elves then assemble those into whole things
-#hadoop","reply_screen_name":"tlipcon",...}</literal></literallayout>
-      <para>Twitter populates the <literal>reply_screen_name</literal>, <literal>reply_tweet_id</literal> and such fields in the JSON record so users can follow the conversation (as you can see, they’re otherwise <literal>null</literal>). Where user A replies to user B, let’s emit A and B separated by a tab:<footnote>
+      <literallayout><literal>{"text":"Just finished the final draft for Hadoop: the Definitive Guide!",
+ "screen_name":"tom_e_white","id":3239897342,"reply_screen_name":null,"reply_tweet_id":null,...}
+{"text":"@tom_e_white Can't wait to get a copy!",
+ "screen_name":"mrflip","id":3239873453,"reply_screen_name":"tom_e_white","reply_tweet_id":3239897342,...}
+{"text":"Lots of ideas buzzing round my head but not quite crystalising, so can't sleep. Want to read Tom's hadoop book, too - thanks @tom_e_white!!",
+ "screen_name":"drsm79","id":3235751418,"reply_screen_name":null,"reply_tweet_id":null,...}
+{"text":"@josephkelly great job on the #InfoChimps API. Remind me to tell you about the time a baboon broke into our house.",
+ "screen_name":"wattsteve","id":16434069252,"reply_screen_name":"josephkelly",...}
+{"text":"@mza Re: http://bit.ly/atbroxmr Have you seen @James_Rubino's http://bit.ly/clusterfork ? Lots of good hadoop refs there too",
+ "screen_name":"mrflip","id":7809927173,"reply_screen_name":"@mza",...}
+{"text":"@tlipcon divide lots of data into little parts. Magic software gnomes fix up the parts, elves then assemble those into whole things #hadoop",
+ "screen_name":"nealrichter","id":4491069515,"reply_screen_name":"tlipcon",...}</literal></literallayout>
+      <para>Twitter populates the <literal>reply_screen_name</literal>, <literal>reply_tweet_id</literal> and such fields in the JSON record so users can follow the conversation (as you can see, they’re otherwise <literal>null</literal>). Where user A replies to user B, let’s emit the edge as A and B separated by a tab:<footnote>
           <para>In practice, we of course use numeric IDs and not screen names, but it’s easier to follow along with screen names. In order to keep the graph-theory discussion general, I’m going to play loose with some details and leave out various janitorial details of loading and running.</para>
         </footnote></para>
       <programlisting>class ReplyGraphMapper &lt; LineStreamer
@@ -161,15 +165,14 @@ DUMP replies_out</programlisting>
       </sect3>
       <sect3>
         <title>Degree</title>
-        <para>A simple, useful measure of influence is the number of replies a user receives. In graph terms this is the <firstterm>degree</firstterm> (specifically, the <firstterm>in-degree</firstterm> since this is a directed graph). </para>
+        <para>A simple, useful measure of influence is the number of replies a user receives. In graph terms this is the <firstterm>degree</firstterm> (specifically the <firstterm>in-degree</firstterm>, since this is a directed graph). </para>
         <para>Pig’s nested FOREACH syntax lets us count the distinct incoming repliers (neighbor nodes) and the total incoming replies in one pass:<footnote>
-            <para>Due to a pesky Hadoop implementation detail, the small size of the edge pair records may inefficiently force the mapper to spill early. If the jobtracker dashboard shows “spilled records” greatly exceeding “map output records,” see if bumping up the <literal>io.sort.record.percent</literal> configuration parameter helps performance:</para>
+            <para>Due to a pesky Hadoop implementation detail, the small size of the edge pair records may inefficiently force the mapper to spill early. If the jobtracker dashboard shows “spilled records” greatly exceeding “map output records,” try bumping up the <literal>io.sort.record.percent</literal> configuration parameter:</para>
             <programlisting>PIG_OPTS="-Dio.sort.record.percent=0.25 -Dio.sort.mb=350" pig my_file.pig</programlisting>
           </footnote></para>
         <programlisting>a_replies_b = LOAD 'a_replies_b' AS (src: chararray, dest:chararray);
-replies_in = GROUP a_replies_b BY dest; -- group on dest to get in-links
-DUMP replies_in
-replies_in_degree  = FOREACH replies_in {
+replies_in  = GROUP a_replies_b BY dest; -- group on dest to get in-links
+replies_in_degree = FOREACH replies_in {
   nbrs = DISTINCT a_replies_b.src;
   GENERATE group, COUNT(nbrs), COUNT(a_replies_b);
 };
@@ -195,11 +198,11 @@ DUMP replies_in_degree</programlisting>
 (bradfordcross,6L,9L)
 (communicating,3L,4L)
 (peteskomoroch,9L,18L)</screen>
-        <para>This shows that <literal>@mrflip</literal>, for example, has five neighbors and 9 incoming replies.</para>
-        <para>In Twitter, the in-degree can vary wildly. Most nodes have very few edges, but a few celebrities—<literal>@THE_REAL_SHAQ</literal> (basketball star Shaquille O’Neill) and <literal>@sockington</literal> (a fictional cat)—have millions. By contrast, almost every intersection on a road map is 4-way.<footnote>
+        <para>In this sample <literal>@mrflip</literal>, for example, has five neighbors and 9 incoming replies.</para>
+        <para>In Twitter, the in-degree can vary wildly. Most nodes have very few edges, but a few celebrities—such as <literal>@THE_REAL_SHAQ</literal> (basketball star Shaquille O’Neill) or <literal>@sockington</literal> (a fictional cat)—have millions. By contrast, almost every intersection on a road map is 4-way.<footnote>
             <para>The largest outlier that comes to mind is the famous "Magic Roundabout" in Swindon, England, with degree 10, <ulink
                 url="http://en.wikipedia.org/wiki/Magic_Roundabout_%28Swindon%29"/>.</para>
-          </footnote> This wild variation in degree (<firstterm>skewness</firstterm>) on the social graph has important ramifications for how you process such graphs—more later.</para>
+          </footnote> The skewed dataflow produced by this wild variation in degree has important ramifications for how you process such graphs—more later.</para>
       </sect3>
     </sect2>
     <sect2>
@@ -207,17 +210,18 @@ DUMP replies_in_degree</programlisting>
       <para>On Twitter, millions of people have given <literal>@THE_REAL_SHAQ</literal> a shout-out; understandably, he has not reciprocated with millions of replies. As the graph shows, I frequently converse with <literal>@mndoci</literal><footnote>
           <para>Deepak Singh, open data advocate and bizdev manager of the Amazon AWS cloud.</para>
         </footnote>, making ours a <firstterm>symmetric link</firstterm>. This accurately reflects the fact that I have more in common with <literal>@mndoci</literal> than with <literal>@THE_REAL_SHAQ</literal>.</para>
-      <para>One line of reasoning says to take the edges in <literal>A Replies B</literal> that are also in <literal>A ReplyFrom B</literal>. We can do that set intersection with an inner self-join:</para>
-      <programlisting>-- out links
-a_replies_b  = LOAD 'a_replies_b.tsv' AS (src:chararray, dest:chararray);
--- in links: reverse each edge
-b_replies_a = FOREACH a_replies_b GENERATE dest AS user_a, src AS user_b;
--- symmetric edges are in both sets
-a_symmetric_b_j = JOIN a_replies_b BY (src, dest), b_replies_a
-  BY (user_a, user_b);</programlisting>
-      <para>However, this sends two copies of the edge-pairs list to the reduce phase, doubling the memory required. We can do better by noticing that from a node’s point of view, a symmetric link is equivalent to one out- and one in-edge. Make the graph undirected by putting the node with lowest sort order in the first slot—but preserve the direction as a piece of edge metadata. </para>
-      <programlisting>a_replies_b    = LOAD 'a_replies_b.tsv' AS (src: chararray, dest:chararray);
-a_b_relations  = FOREACH a_replies_b GENERATE
+      <para>One line of reasoning says to take the edges in <literal>A Replies B</literal> that are also in <literal>A ReplyFrom B</literal>. We can do that set intersection with an inner self-join<footnote>
+          <para>Current versions of Pig get confused on self-joins, so just load the table with differently-named relations as shown here.</para>
+        </footnote>:</para>
+      <programlisting>a_replies_b = LOAD 'a_replies_b.tsv' AS (user_a:chararray, user_b:chararray);
+a_repl_by_b = LOAD 'a_replies_b.tsv' AS (user_b:chararray, user_a:chararray);
+-- symmetric edges appear in both sets
+a_symmetric_b_j = JOIN a_replies_b BY (user_a, user_b),
+                       a_repl_by_b BY (user_a, user_b);
+...</programlisting>
+      <para>However, this sends two full copies of the edge-pairs list to the reduce phase, doubling the memory required. We can do better by noticing that from a node’s point of view, a symmetric link is equivalent to a paired edge: one out and one in. Make the graph undirected by putting the node with lowest sort order in the first slot—but preserve the direction as a piece of edge metadata:</para>
+      <programlisting>a_replies_b = LOAD 'a_replies_b.tsv' AS (src: chararray, dest:chararray);
+a_b_relations = FOREACH a_replies_b GENERATE
   ((src &lt;= dest) ? src  : dest) AS user_a,
   ((src &lt;= dest) ? dest : src)  AS user_b,
   ((src &lt;= dest) ? 1 : 0)       AS a_re_b:int,
@@ -227,21 +231,21 @@ DUMP a_b_relations</programlisting>
 (josephkelly,wattsteve,0,1)
 (mrflip,mza,1,0)
 (nealrichter,tlipcon,0,1)</screen>
-      <para>Now gather all edges for each node pair together, and check for symmetry:</para>
+      <para>Now gather all edges for each node pair. A symmetric edge has at least one reply in each direction:</para>
       <programlisting>a_b_relations_g   = GROUP a_b_relations BY (user_a, user_b);
-a_symmetric_b_all = FOREACH a_b_relations_g GENERATE 
+a_b_symm_all = FOREACH a_b_relations_g GENERATE 
   group.user_a AS user_a, 
   group.user_b AS user_b,
   ( ((SUM(a_b_relations.a_re_b) > 0) AND (SUM(a_b_relations.b_re_a) > 0)) ? 1 : 0)
     AS is_symmetric:int;
-DUMP a_symmetric_b</programlisting>
+DUMP a_b_symm_all</programlisting>
       <screen>(mrflip,tom_e_white,1)
 (mrflip,mza,0)
 (josephkelly,wattsteve,0)
 (nealrichter,tlipcon,1)
 ...</screen>
-      <programlisting>a_symmetric_b = FILTER a_symmetric_b_all BY (is_symmetric == 1);
-STORE a_symmetric_b INTO 'a_symmetric_b.tsv';</programlisting>
+      <programlisting>a_b_symm = FILTER a_b_symm_all BY (is_symmetric == 1);
+STORE a_b_symm INTO 'a_b_symm.tsv';</programlisting>
       <para>Here’s a portion of the output, showing that <literal>@mrflip</literal> and <literal>@tom_e_white</literal> have a symmetric link:</para>
       <screen>(mrflip,tom_e_white,1)
 (nealrichter,tlipcon,1)
@@ -253,7 +257,7 @@ STORE a_symmetric_b INTO 'a_symmetric_b.tsv';</programlisting>
       <sect3>
         <title>Get neighbors</title>
         <para>Choose a seed node. First round up the seed’s neighbors:</para>
-        <programlisting>a_replies_b    = LOAD 'a_replies_b.tsv' AS (src: chararray, dest:chararray);
+        <programlisting>a_replies_b = LOAD 'a_replies_b.tsv' AS (src: chararray, dest:chararray);
 -- Extract edges that originate or terminate on the seed
 n0_edges = FILTER a_replies_b BY (src == 'hadoop') OR (dest == 'hadoop');
 -- Choose the node in each pair that *isn't* our seed:
@@ -262,12 +266,12 @@ n1_nodes_all = FOREACH n0_edges GENERATE ((src == 'hadoop') ? dest : src)
 n1_nodes    = DISTINCT n1_nodes_all;
 DUMP n1_nodes</programlisting>
         <para>Now intersect the set of neighbors with the set of starting nodes to find all edges originating in <literal>n1_nodes</literal>:</para>
-        <programlisting>n1_edges_left_j    = JOIN a_replies_b BY src, n1_nodes
-  BY screen_name USING 'replicated';
-n1_edges_left      = FOREACH n1_edges_left_j GENERATE src, dest;</programlisting>
-        <para>Our copy of the graph has more than 1 billion edges, far too large to fit in memory. On the other hand, the neighbor count for a single user rarely exceeds a couple million, which fits easily in memory. Including <literal>USING 'replicated'</literal> in the JOIN command instructs Pig to do a map-side join (also called a <firstterm>fragment replicate join</firstterm>). Pig holds the <literal>n0_nodes</literal> relation in memory as a lookup table, and streams the full edge list past. Whenever the join condition is met—<literal>user_a</literal> is in the <literal>n0_nodes</literal> lookup table—it produces output. No reduce step makes for an enormous speedup!</para>
-        <para>Finally, take all the edges out from {} and retain only those ending in to {Nbrs}.</para>
-        <programlisting>-- Among those edges, find those that terminate in n1_nodes as well
+        <programlisting>n1_edges_left_j = JOIN a_replies_b BY src, n1_nodes
+                  BY screen_name USING 'replicated';
+n1_edges_left   = FOREACH n1_edges_left_j GENERATE src, dest;</programlisting>
+        <para>Our copy of the graph (with more than 1 billion edges) is far too large to fit in memory. On the other hand, the neighbor count for a single user rarely exceeds a couple million, which fits easily in memory. Including <literal>USING 'replicated'</literal> in the JOIN command instructs Pig to do a map-side join (also called a <firstterm>fragment replicate join</firstterm>). Pig holds the <literal>n1_nodes</literal> relation in memory as a lookup table, and streams the full edge list past. Whenever the join condition is met—<literal>src</literal> is in the <literal>n1_nodes</literal> lookup table—it produces output. With no reduce step we see an enormous speedup!</para>
+        <para>Lastly repeat the join, to leave only edges whose source and destination are neighbors of the seed node:</para>
+        <programlisting>
 n1_edges_j = JOIN n1_edges_left BY dest, n1_nodes
   BY screen_name USING 'replicated';
 n1_edges    = FOREACH n1_edges_j GENERATE src, dest;
@@ -282,12 +286,11 @@ DUMP n1_edges</programlisting>
 (nealrichter,datajunkie)</screen>
       </sect3>
       <sect3>
-        <title>Community metrics and the 1 billion x 1 billion problem</title>
-        <para><xref linkend="BigDataTwitterCommunity"
-            /> displays the community extracted using <literal>@hadoop</literal>, <literal
+        <title>Community metrics and the 1 million x 1 million problem</title>
+        <para>With <literal>@hadoop</literal>, <literal
             >@cloudera</literal> and <literal
-            >@infochimps</literal> as seeds. As you can see, the big data community is very interconnected. The link neighborhood of a celebrity such as <literal
-            >@THE_REAL_SHAQ</literal> is far more sparse.</para>
+            >@infochimps</literal> as seeds, I applied similar scripts to 2 billion messages to create <xref linkend="BigDataTwitterCommunity"
+            />.</para>
         <figure id="BigDataTwitterCommunity">
           <title>Big data community on Twitter</title>
           <mediaobject>
@@ -296,18 +299,18 @@ DUMP n1_edges</programlisting>
             </imageobject>
           </mediaobject>
         </figure>
-        <para>We can characterize this using the <firstterm>clustering coefficient</firstterm>: the number of <literal>n1_edges</literal> to the maximum number of possible <literal>n1_edges</literal>. It ranges from zero (no neighbor links to any other neighbor) to one (every neighbor links to every other neighbor). A high clustering coefficient indicates a cohesive community. A low clustering coefficient could indicate widely-dispersed interest (as it does with <literal>@THE_REAL_SHAQ</literal>), or it could indicate the kind of inorganic community that a spam account would engender.</para>
-        <para>Don’t try to recklessly generalize the calculation above to the full dataset. Remember the wide variation in node degree on the social graph discussed above? This skewness leads to an explosion of data—pop star <literal>@britneyspears</literal> (5.2M followers, 420k following as of July 2010) or <literal>@WholeFoods</literal> (1.7M followers, 600k following) will each generate trillions of entries. What’s worse, almost all of these will be thrown away: as just argued, the clustering coefficient for celebrities is very low! There is a very elegant way to do this on the full graph<footnote>
+        <para>As you can see, the big data community is very interconnected. The link neighborhood of a celebrity such as <literal>@THE_REAL_SHAQ</literal> is far more sparse. We can characterize this using the <firstterm>clustering coefficient</firstterm>: the ratio of actual <literal>n1_edges</literal> to the maximum number of possible <literal>n1_edges</literal>. It ranges from zero (no neighbor links to any other neighbor) to one (every neighbor links to every other neighbor). A moderately high clustering coefficient indicates a cohesive community. A low clustering coefficient could indicate widely-dispersed interest (as it does with <literal>@THE_REAL_SHAQ</literal>), or it could indicate the kind of inorganic community that a spam account would engender.</para>
+        <para>Simultaneously calculating the clustering coefficient for the full dataset requires some care. Remember the wide variation in node degree discussed above? Recklessly extending the method above will lead to an explosion of data—pop star <literal>@britneyspears</literal> (5.2M followers, 420k following as of July 2010) or <literal>@WholeFoods</literal> (1.7M followers, 600k following) will each generate trillions of entries. What’s worse, since large communities have a sparse clustering coefficient almost all of these will be thrown away! There is a very elegant way to do this on the full graph<footnote>
             <para>See <ulink url="http://www.slideshare.net/ydn/3-xxl-graphalgohadoopsummit2010"
-                />, one of many fine tricks by Jake Hofman (<literal
-                >@jakehofman</literal>) and Sergei Vassilvitskii (<literal
-              >@vsergei</literal>) of Yahoo! Research.</para>
-          </footnote>, but, if you’re willing to assume that <literal>@britneyspears</literal> isn’t <emphasis
+                />—Sergei Vassilvitskii (<literal
+              >@vsergei</literal>) and Jake Hofman (<literal
+                >@jakehofman</literal>) of Yahoo! Research solve several graph problems by very intelligently throwing away most of the graph.</para>
+          </footnote>, but always keep in mind what the real world says about the problem. Keep only the <firstterm>strong links</firstterm>: weight each edge (by number of replies, whether it’s symmetric, and so on) and set limits on the number of links from any node. If you’re willing to assert that <literal>@britneyspears</literal> isn’t <emphasis
             role="italic"
-          >really</emphasis> friends with 420,000 people, you can still do a reasonable job. Just weight each edge (by number of replies, whether it’s symmetric, and so on) and set limits on the number of links from any node.</para>
+          >really</emphasis> friends with 420,000 people, you can sharply reduce the intermediate data size and still do a reasonable job.</para>
         <para>
         </para>
-        <para role="right">—Philip (flip) Kromer</para>
+        <para role="right">—Philip (flip) Kromer, Infochimps</para>
       </sect3>
     </sect2>
   </sect1>

commit 5601e39e3abb54b71bea7cef988e1d6f572a5c28
Author: Philip (flip) Kromer <flip@infochimps.org>
Date:   Sun Jul 25 22:18:31 2010 -0500

    Copy changes to hadoop chapter

diff --git a/blog_posts/hadoop_book/flip.xml b/blog_posts/hadoop_book/flip.xml
index 4c0e1ac..ec3ddd0 100644
--- a/blog_posts/hadoop_book/flip.xml
+++ b/blog_posts/hadoop_book/flip.xml
@@ -8,11 +8,17 @@
     <para>Using Pig and Wukong to Explore Billion-edge Network Graphs</para>
     <sect2>
       <title>Basics</title>
-      <para>Networks at massive scale are fascinating. It’s a very general model: you have things (that we’ll call nodes); they’re related (edges); and we can attach numbers or attributes to those things or relationships (node or edge metadata).</para>
-      <para>For example, here are a few datasets that come up in a search for “network” on infochimps.org:</para>
+      <para>Networks at massive scale are fascinating. The number of things they model are extremely general: if you have a collection of things (that we'll call nodes), they are related (edges), and the nodes and edges tell a story (node / edge metadata), you have a network graph.</para>
+      <para>I started the Infochimps project, a site to find, share or sell ay dataset in the world. At Infochimps, we’ve got a whole bag of tricks ready to apply to any interesting network graph that comes into the collection. We chiefly use Pig (described in chapter XXXX) and Wukong (<ulink
+          url="http://github.com/mrflip/wukong"
+        />), a toolkit we've developed for Hadoop streaming in the Ruby programming language. They let us write simple scripts like the ones below—almost all of which fit on a single printed page—to process networks as large as 100 million nodes and 2 billion edges.  Here are a few datasets that come up in a search for “network” on infochimps.org<footnote>
+              <para><ulink
+                  url="http://infochimps.org/search?query=network"
+                /></para>
+            </footnote>:</para>
       <itemizedlist>
         <listitem>
-          <para>A social network, such as Twitter or Facebook. People are somewhat impersonally modeled as nodes, and relationships (<literal>@mrflip</literal> is friends with <literal>@tom_e_white</literal>) or actions (<literal>@infochimps</literal> mentioned <literal>@hadoop</literal>) as edges. The number of messages a user has sent and the bag of words from all those messages are each pieces of node metadata.</para>
+          <para>A social network, such as Twitter or Facebook.  We somewhat impersonally model people as nodes, and relationships (<literal>@mrflip</literal> is friends with <literal>@tom_e_white</literal>) or actions (<literal>@infochimps</literal> replied to <literal>@hadoop</literal>) as edges. The number of messages a user has sent and the bag of words from all those messages are each important pieces of node metadata.</para>
         </listitem>
         <listitem>
           <para>A linked document collection such as Wikipedia or the entire web. Each page is a node (carrying its title, view count and categories as node metadata). Each hyperlink is an edge, and the frequency at which people click from one page to the next is edge metadata.</para>
@@ -21,19 +27,18 @@
           <para>The connections of neurons (nodes) and synapses (edges) in the <emphasis
               role="italic">C. elegans</emphasis> roundworm.<footnote>
               <para><ulink
-                  url="http://infochimps.org/datasets/neuronal-wiring-network-of-the-caenorhabditis-elegans-roundworm"
+                  url="http://www.wormatlas.org/neuronalwiring.html"
                 /></para>
             </footnote></para>
         </listitem>
         <listitem>
           <para>A highway map, with exits as nodes, and highway segments as edges. The Open Street Map project’s dataset has global coverage of place names (node metadata), street number ranges (edge metadata), and more.<footnote>
-              <para><ulink url="http://www.openstreetmap.org/"/>, <ulink
-                  url="http://infochimps.org/datasets/open-street-map"/></para>
+              <para><ulink url="http://www.openstreetmap.org/"/></para>
             </footnote></para>
         </listitem>
+      <listitem>Or more esoteric graphs that fall out when you take aninteresting problem and shake it just right. Take a few million Twitter messagesand emit an edge for every pair of non-keyboard characters within the samemessage. Simply by observing “often, when humans use 最, they also use 近” youcan recreate a map of human languages (see <xref
+          linkend="TwitterLanguageMap"/>).</listitem>
       </itemizedlist>
-      <para>Or more esoteric graphs that fall out when you take an interesting problem and shake it just right. Take a few million Twitter messages and, every time a non-keyboard character is used, emit an edge for every other such character in the message. Simply by saying “sometimes, when humans use 最, they often use 近” you can recreate a map of human languages (see <xref
-          linkend="TwitterLanguageMap"/>).</para>
       <figure id="TwitterLanguageMap">
         <title>Twitter Language Map</title>
         <mediaobject>
@@ -44,7 +49,7 @@
       </figure>
       <para>What’s amazing about these organic network graphs is that given enough data, a growing collection of powerful tools are able to <emphasis
           role="italic">generically</emphasis> use this network structure to expose insight. For example, we’ve used variants of the same algorithm<footnote>
-          <para>All are steady-state network flow problems. A huge number of random websurfers wandering through the linked-document collection will visit the most interesting pages most often. If you model social network interactions as exchanges of social capital, the steady-state flow of that capital will highlight the most interesting users. The year-to-year progress of students to higher or lower scores implies what each school’s effect on a generic class would be.</para>
+          <para>All are steady-state network flow problems. A flowing crowd of websurfers wandering through the linked-document collection will visit the most interesting pages most often. The steady-state transfer of social capital implied by social network interactions will highlight the most central actors within each community. The year-to-year progress of students to higher or lower test scores implies what each school’s effect on a generic class would be.</para>
         </footnote> to do each of:</para>
       <itemizedlist>
         <listitem>
@@ -57,13 +62,10 @@
           <para>Predict a school’s impact on student education using millions of anonymized exam scores gathered over five years.</para>
         </listitem>
       </itemizedlist>
-      <para>At Infochimps, we’ve got a whole bag of tricks ready to apply to any interesting network graph that comes into the collection. We chiefly use Pig (described in chapter XXXX) and Wukong (<ulink
-          url="http://github.com/mrflip/wukong"
-        />), a toolkit we've developed for Hadoop streaming in the Ruby programming language. They let use write simple scripts like the ones below—almost all of which fit on a single printed page—that capably process networks of 100 million nodes and 2 billion edges.</para>
     </sect2>
     <sect2>
       <title>Measuring Community</title>
-      <para>The most interesting network in our collection is a massive crawl of the Twitter social graph. With more than 50 million nodes and 2 billion edges, it is a fantastic instrument for understanding what people talk about and how they relate to each other. Here are straightforward ways to characterize a twitter user’s community: </para>
+      <para>The most interesting network in the Infochimps collection is a massive crawl of the Twitter social graph. With more than 50 million nodes and 2 billion edges, it is a marvelous instrument for understanding what people talk about and how they relate to each other. Here are straightforward ways to characterize a twitter user’s community: </para>
       <itemizedlist>
         <listitem>
           <para>Who are the people they converse with (the @reply graph)?</para>
@@ -76,7 +78,7 @@
         </listitem>
       </itemizedlist>
       <para>Here’s an exploration of these questions within the Twitter subuniverse of “People who talk about Infochimps or Hadoop”.<footnote>
-          <para>In keeping with the ego-centered ethos of social networks, chosen without apology.</para>
+          <para>Chosen without apology, in keeping with the ego-centered ethos of social networks.</para>
         </footnote></para>
     </sect2>
     <sect2>

commit fd4d8ebfb01beb288b1810dcc71b338400bf8623
Author: Philip (flip) Kromer <flip@infochimps.org>
Date:   Sun Jul 25 21:42:57 2010 -0500

    Draft of book in docbook format back from Tom White

diff --git a/blog_posts/hadoop_book/flip.xml b/blog_posts/hadoop_book/flip.xml
new file mode 100644
index 0000000..4c0e1ac
--- /dev/null
+++ b/blog_posts/hadoop_book/flip.xml
@@ -0,0 +1,312 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
+"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">
+<chapter id="flip">
+  <title>Case Studies</title>
+  <sect1>
+    <title>Graph Analysis Chimpanzee Style</title>
+    <para>Using Pig and Wukong to Explore Billion-edge Network Graphs</para>
+    <sect2>
+      <title>Basics</title>
+      <para>Networks at massive scale are fascinating. It’s a very general model: you have things (that we’ll call nodes); they’re related (edges); and we can attach numbers or attributes to those things or relationships (node or edge metadata).</para>
+      <para>For example, here are a few datasets that come up in a search for “network” on infochimps.org:</para>
+      <itemizedlist>
+        <listitem>
+          <para>A social network, such as Twitter or Facebook. People are somewhat impersonally modeled as nodes, and relationships (<literal>@mrflip</literal> is friends with <literal>@tom_e_white</literal>) or actions (<literal>@infochimps</literal> mentioned <literal>@hadoop</literal>) as edges. The number of messages a user has sent and the bag of words from all those messages are each pieces of node metadata.</para>
+        </listitem>
+        <listitem>
+          <para>A linked document collection such as Wikipedia or the entire web. Each page is a node (carrying its title, view count and categories as node metadata). Each hyperlink is an edge, and the frequency at which people click from one page to the next is edge metadata.</para>
+        </listitem>
+        <listitem>
+          <para>The connections of neurons (nodes) and synapses (edges) in the <emphasis
+              role="italic">C. elegans</emphasis> roundworm.<footnote>
+              <para><ulink
+                  url="http://infochimps.org/datasets/neuronal-wiring-network-of-the-caenorhabditis-elegans-roundworm"
+                /></para>
+            </footnote></para>
+        </listitem>
+        <listitem>
+          <para>A highway map, with exits as nodes, and highway segments as edges. The Open Street Map project’s dataset has global coverage of place names (node metadata), street number ranges (edge metadata), and more.<footnote>
+              <para><ulink url="http://www.openstreetmap.org/"/>, <ulink
+                  url="http://infochimps.org/datasets/open-street-map"/></para>
+            </footnote></para>
+        </listitem>
+      </itemizedlist>
+      <para>Or more esoteric graphs that fall out when you take an interesting problem and shake it just right. Take a few million Twitter messages and, every time a non-keyboard character is used, emit an edge for every other such character in the message. Simply by saying “sometimes, when humans use 最, they often use 近” you can recreate a map of human languages (see <xref
+          linkend="TwitterLanguageMap"/>).</para>
+      <figure id="TwitterLanguageMap">
+        <title>Twitter Language Map</title>
+        <mediaobject>
+          <imageobject>
+            <imagedata fileref="diagrams/twitter_language_map-labels.png" format="PNG"/>
+          </imageobject>
+        </mediaobject>
+      </figure>
+      <para>What’s amazing about these organic network graphs is that given enough data, a growing collection of powerful tools are able to <emphasis
+          role="italic">generically</emphasis> use this network structure to expose insight. For example, we’ve used variants of the same algorithm<footnote>
+          <para>All are steady-state network flow problems. A huge number of random websurfers wandering through the linked-document collection will visit the most interesting pages most often. If you model social network interactions as exchanges of social capital, the steady-state flow of that capital will highlight the most interesting users. The year-to-year progress of students to higher or lower scores implies what each school’s effect on a generic class would be.</para>
+        </footnote> to do each of:</para>
+      <itemizedlist>
+        <listitem>
+          <para>Rank the most important pages in the Wikipedia linked-document collection. Google uses a vastly more refined version this approach to identify top search hits.</para>
+        </listitem>
+        <listitem>
+          <para>Identify celebrities and experts in the Twitter social graph. Users who have many more followers than their “trstrank” would imply are often spammers.</para>
+        </listitem>
+        <listitem>
+          <para>Predict a school’s impact on student education using millions of anonymized exam scores gathered over five years.</para>
+        </listitem>
+      </itemizedlist>
+      <para>At Infochimps, we’ve got a whole bag of tricks ready to apply to any interesting network graph that comes into the collection. We chiefly use Pig (described in chapter XXXX) and Wukong (<ulink
+          url="http://github.com/mrflip/wukong"
+        />), a toolkit we've developed for Hadoop streaming in the Ruby programming language. They let use write simple scripts like the ones below—almost all of which fit on a single printed page—that capably process networks of 100 million nodes and 2 billion edges.</para>
+    </sect2>
+    <sect2>
+      <title>Measuring Community</title>
+      <para>The most interesting network in our collection is a massive crawl of the Twitter social graph. With more than 50 million nodes and 2 billion edges, it is a fantastic instrument for understanding what people talk about and how they relate to each other. Here are straightforward ways to characterize a twitter user’s community: </para>
+      <itemizedlist>
+        <listitem>
+          <para>Who are the people they converse with (the @reply graph)?</para>
+        </listitem>
+        <listitem>
+          <para>Do the people they engage with reciprocate that attention (symmetric links)?</para>
+        </listitem>
+        <listitem>
+          <para>Among the user’s community, how many engage with each other (clustering coefficient)?</para>
+        </listitem>
+      </itemizedlist>
+      <para>Here’s an exploration of these questions within the Twitter subuniverse of “People who talk about Infochimps or Hadoop”.<footnote>
+          <para>In keeping with the ego-centered ethos of social networks, chosen without apology.</para>
+        </footnote></para>
+    </sect2>
+    <sect2>
+      <title>Everybody’s Talkin’ At Me: The Twitter Reply Graph</title>
+      <para>Twitter lets you reply to another user’s message and thus engage in conversation. Since it’s an expressly public activity, a reply is a strong <emphasis
+          role="italic"
+        >social token</emphasis>: it shows interest in what the other is saying, and demonstrates that interest is worth re-broadcasting.</para>
+      <para>The first step in our processing is done in Wukong, a Ruby language library for Hadoop. It lets us write small, agile programs capable of handling multi-terabyte data streams. Here is a snippet from the class that represents a twitter message:<footnote>
+          <para>You may find full working source code on the book’s website.</para>
+        </footnote></para>
+      <programlisting>class Tweet &lt; Struct.new(:tweet_id, :screen_name, :created_at,
+    :reply_tweet_id, :reply_screen_name, :text)
+  def initialize(raw_tweet)
+    # ... gory details of parsing raw tweet omitted
+  end
+  
+  # Tweet is a reply if there's something in the reply_tweet_id slot
+  def is_reply?
+    not reply_tweet_id.blank?
+  true
+end</programlisting>
+      <para>Anyone can pull gigabytes of data per day from Twitter’s streaming API: follow the instructions at <ulink
+          url="http://dev.twitter.com/doc/get/statuses/sample"/> or use a tool like <ulink
+          url="http://github.com/hayesdavis/flamingo"/>.</para>
+      <para>Twitter messages typically arrive in raw JSON format. Here are a few tweets:</para>
+      <literallayout><literal>{"id":3239897342,"screen_name":"tom_e_white","text":"Just finished the final draft for Hadoop: the Definitive Guide!","reply_screen_name":null,"reply_tweet_id":null,...}
+{"id":3239873453,"screen_name":"mrflip","text":"@tom_e_white Can't wait to get a copy!","reply_screen_name":"tom_e_white","reply_tweet_id":3239897342,...}
+{"id":3235751418,"screen_name":"drsm79","text":"Lots of ideas buzzing round my head but not quite crystalising, so can't sleep. Want to read Tom's hadoop book, too - thanks @tom_e_white!!","reply_screen_name":null,"reply_tweet_id":null,...}
+{"id":16434069252,"screen_name":"wattsteve","text":"@josephkelly great job on the #InfoChimps API. Next time I come by remind me to tell you about the time a baboon broke into our house.","reply_screen_name":"josephkelly",...}
+{"id":7809927173,"screen_name":"mrflip","text":"@mza Re: http://bit.ly/atbroxmr Have you seen @James_Rubino's http://bit.ly/clusterfork ? Lots of good hadoop refs there too","reply_screen_name":"@mza",...}
+{"id":4491069515,"screen_name":"nealrichter","text":"@tlipcon divide lots of data into little parts.
+Magic software gnomes fix up the parts, elves then assemble those into whole things
+#hadoop","reply_screen_name":"tlipcon",...}</literal></literallayout>
+      <para>Twitter populates the <literal>reply_screen_name</literal>, <literal>reply_tweet_id</literal> and such fields in the JSON record so users can follow the conversation (as you can see, they’re otherwise <literal>null</literal>). Where user A replies to user B, let’s emit A and B separated by a tab:<footnote>
+          <para>In practice, we of course use numeric IDs and not screen names, but it’s easier to follow along with screen names. In order to keep the graph-theory discussion general, I’m going to play loose with some details and leave out various janitorial details of loading and running.</para>
+        </footnote></para>
+      <programlisting>class ReplyGraphMapper &lt; LineStreamer
+  def process(raw_tweet)
+    tweet = Tweet.new(raw_tweet)
+    if tweet.is_reply?
+      emit [tweet.screen_name, tweet.reply_screen_name]
+    end
+  end
+end</programlisting>
+      <para>The mapper derives from <literal>LineStreamer</literal>, a Wukong class that calls its <literal>process</literal> method on each line as a single record. It handles all the routing; we only have to define that <literal>process</literal> method. In this case, we use the raw JSON record to create a tweet object. When that tweet is part of a conversation, we emit the respective user IDs as an edge. Here’s what the raw output will look like:</para>
+      <screen><prompt>% </prompt><userinput>reply_graph_mapper --run raw_tweets.json a_replies_b.tsv</userinput>
+mrflip          tom_e_white
+wattsteve       josephkelly
+mrflip          mza
+nealrichter     tlipcon</screen>
+      <para>You should read this as “a replies b”, and interpret it as a directed “out” edge: <literal>@wattsteve</literal> conveys social capital to <literal>@josephkelly</literal>. </para>
+      <sect3>
+        <title>Edge pairs</title>
+        <para>This is the <firstterm>edge pairs</firstterm> representation of a network. It’s simple, and it gives an equal jumping-off point for in- or out- edges, but there’s some duplication of data. You can tell the same story from the node’s point of view (and save some disk space) by rolling up on the source node. In Pig, this is a simple GROUP BY. Load the file:</para>
+        <programlisting>a_replies_b = LOAD 'a_replies_b' AS (src: chararray, dest:chararray);</programlisting>
+        <para>Then find all edges out from each node by grouping on source:</para>
+        <programlisting>replies_out = GROUP a_replies_b BY source;
+DUMP replies_out</programlisting>
+        <screen>(CMastication,{(esammer,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(esammer,CMastication),(esammer,CMastication),(ChrisDiehl,CMastication),(esammer,CMastication),(peteskomoroch,CMastication),(esammer,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(peteskomoroch,CMastication),(esammer,CMastication),(bradfordcross,CMastication),(peteskomoroch,CMastication)})
+(ChrisDiehl,{(DataJunkie,ChrisDiehl),(DataJunkie,ChrisDiehl),(peteskomoroch,ChrisDiehl)})
+(DataJunkie,{(CMastication,DataJunkie),(CMastication,DataJunkie),(nealrichter,DataJunkie),(peteskomoroch,DataJunkie),(mat_kelcey,DataJunkie),(CMastication,DataJunkie),(communicating,DataJunkie),(ChrisDiehl,DataJunkie),(bradfordcross,DataJunkie),(nealrichter,DataJunkie),(mat_kelcey,DataJunkie),(mat_kelcey,DataJunkie)})
+(LusciousPear,{(bradfordcross,LusciousPear),(metcalfc,LusciousPear),(mikeolson,LusciousPear),(metcalfc,LusciousPear),(metcalfc,LusciousPear),(tlipcon,LusciousPear),(kevinweil,LusciousPear),(bradfordcross,LusciousPear),(bradfordcross,LusciousPear),(DataJunkie,LusciousPear),(esammer,LusciousPear)})
+(bradfordcross,{(LusciousPear,bradfordcross),(CMastication,bradfordcross),(mrflip,bradfordcross),(LusciousPear,bradfordcross),(peteskomoroch,bradfordcross),(ogrisel,bradfordcross),(LusciousPear,bradfordcross),(DataJunkie,bradfordcross),(ogrisel,bradfordcross)})
+(communicating,{(nealrichter,communicating),(DataJunkie,communicating),(ogrisel,communicating),(ogrisel,communicating)})
+(cutting,{(tom_e_white,cutting)})
+(esammer,{(CMastication,esammer),(CMastication,esammer),(mikeolson,esammer),(mikeolson,esammer),(jeromatron,esammer)})
+(jeromatron,{(mrflip,jeromatron),(mrflip,jeromatron)})
+(josephkelly,{(wattsteve,josephkelly)})
+(kevinweil,{(tlipcon,kevinweil),(DataJunkie,kevinweil),(mikeolson,kevinweil),(LusciousPear,kevinweil),(tlipcon,kevinweil),(tlipcon,kevinweil),(DataJunkie,kevinweil),(mrflip,kevinweil),(LusciousPear,kevinweil)})
+(metcalfc,{(nealrichter,metcalfc),(nealrichter,metcalfc),(nealrichter,metcalfc)})
+(mikeolson,{(LusciousPear,mikeolson),(kevinweil,mikeolson),(LusciousPear,mikeolson),(tlipcon,mikeolson)})
+(mndoci,{(mrflip,mndoci),(peteskomoroch,mndoci),(LusciousPear,mndoci),(mrflip,mndoci)})
+(mrflip,{(LusciousPear,mrflip),(mndoci,mrflip),(mndoci,mrflip),(ogrisel,mrflip),(esammer,mrflip),(ogrisel,mrflip),(esammer,mrflip),(esammer,mrflip),(wattsteve,mrflip)})
+(mza,{(mrflip,mza)})
+(nealrichter,{(metcalfc,nealrichter),(DataJunkie,nealrichter),(peteskomoroch,nealrichter),(metcalfc,nealrichter)})
+(ogrisel,{(mrflip,ogrisel),(bradfordcross,ogrisel),(mrflip,ogrisel)})
+(peteskomoroch,{(CMastication,peteskomoroch),(esammer,peteskomoroch),(DataJunkie,peteskomoroch),(CMastication,peteskomoroch),(mndoci,peteskomoroch),(CMastication,peteskomoroch),(nealrichter,peteskomoroch),(nealrichter,peteskomoroch),(bradfordcross,peteskomoroch),(ChrisDiehl,peteskomoroch),(mrflip,peteskomoroch),(mrflip,peteskomoroch),(LusciousPear,peteskomoroch),(bradfordcross,peteskomoroch),(ChrisDiehl,peteskomoroch),(nealrichter,peteskomoroch),(CMastication,peteskomoroch),(mndoci,peteskomoroch)})
+(tlipcon,{(mrflip,tlipcon),(LusciousPear,tlipcon),(LusciousPear,tlipcon),(nealrichter,tlipcon),(LusciousPear,tlipcon),(nealrichter,tlipcon),(mrflip,tlipcon),(kevinweil,tlipcon)})
+(tom_e_white,{(mrflip,tom_e_white),(lenbust,tom_e_white)})</screen>
+      </sect3>
+      <sect3>
+        <title>Degree</title>
+        <para>A simple, useful measure of influence is the number of replies a user receives. In graph terms this is the <firstterm>degree</firstterm> (specifically, the <firstterm>in-degree</firstterm> since this is a directed graph). </para>
+        <para>Pig’s nested FOREACH syntax lets us count the distinct incoming repliers (neighbor nodes) and the total incoming replies in one pass:<footnote>
+            <para>Due to a pesky Hadoop implementation detail, the small size of the edge pair records may inefficiently force the mapper to spill early. If the jobtracker dashboard shows “spilled records” greatly exceeding “map output records,” see if bumping up the <literal>io.sort.record.percent</literal> configuration parameter helps performance:</para>
+            <programlisting>PIG_OPTS="-Dio.sort.record.percent=0.25 -Dio.sort.mb=350" pig my_file.pig</programlisting>
+          </footnote></para>
+        <programlisting>a_replies_b = LOAD 'a_replies_b' AS (src: chararray, dest:chararray);
+replies_in = GROUP a_replies_b BY dest; -- group on dest to get in-links
+DUMP replies_in
+replies_in_degree  = FOREACH replies_in {
+  nbrs = DISTINCT a_replies_b.src;
+  GENERATE group, COUNT(nbrs), COUNT(a_replies_b);
+};
+DUMP replies_in_degree</programlisting>
+        <screen>(mza,1L,1L)
+(mndoci,3L,4L)
+(mrflip,5L,9L)
+(cutting,1L,1L)
+(esammer,3L,5L)
+(ogrisel,2L,3L)
+(tlipcon,4L,8L)
+(metcalfc,1L,3L)
+(kevinweil,5L,9L)
+(mikeolson,3L,4L)
+(ChrisDiehl,2L,3L)
+(DataJunkie,7L,12L)
+(jeromatron,1L,2L)
+(josephkelly,1L,1L)
+(nealrichter,3L,4L)
+(tom_e_white,2L,2L)
+(CMastication,4L,16L)
+(LusciousPear,7L,11L)
+(bradfordcross,6L,9L)
+(communicating,3L,4L)
+(peteskomoroch,9L,18L)</screen>
+        <para>This shows that <literal>@mrflip</literal>, for example, has five neighbors and 9 incoming replies.</para>
+        <para>In Twitter, the in-degree can vary wildly. Most nodes have very few edges, but a few celebrities—<literal>@THE_REAL_SHAQ</literal> (basketball star Shaquille O’Neill) and <literal>@sockington</literal> (a fictional cat)—have millions. By contrast, almost every intersection on a road map is 4-way.<footnote>
+            <para>The largest outlier that comes to mind is the famous "Magic Roundabout" in Swindon, England, with degree 10, <ulink
+                url="http://en.wikipedia.org/wiki/Magic_Roundabout_%28Swindon%29"/>.</para>
+          </footnote> This wild variation in degree (<firstterm>skewness</firstterm>) on the social graph has important ramifications for how you process such graphs—more later.</para>
+      </sect3>
+    </sect2>
+    <sect2>
+      <title>Symmetric Links</title>
+      <para>On Twitter, millions of people have given <literal>@THE_REAL_SHAQ</literal> a shout-out; understandably, he has not reciprocated with millions of replies. As the graph shows, I frequently converse with <literal>@mndoci</literal><footnote>
+          <para>Deepak Singh, open data advocate and bizdev manager of the Amazon AWS cloud.</para>
+        </footnote>, making ours a <firstterm>symmetric link</firstterm>. This accurately reflects the fact that I have more in common with <literal>@mndoci</literal> than with <literal>@THE_REAL_SHAQ</literal>.</para>
+      <para>One line of reasoning says to take the edges in <literal>A Replies B</literal> that are also in <literal>A ReplyFrom B</literal>. We can do that set intersection with an inner self-join:</para>
+      <programlisting>-- out links
+a_replies_b  = LOAD 'a_replies_b.tsv' AS (src:chararray, dest:chararray);
+-- in links: reverse each edge
+b_replies_a = FOREACH a_replies_b GENERATE dest AS user_a, src AS user_b;
+-- symmetric edges are in both sets
+a_symmetric_b_j = JOIN a_replies_b BY (src, dest), b_replies_a
+  BY (user_a, user_b);</programlisting>
+      <para>However, this sends two copies of the edge-pairs list to the reduce phase, doubling the memory required. We can do better by noticing that from a node’s point of view, a symmetric link is equivalent to one out- and one in-edge. Make the graph undirected by putting the node with lowest sort order in the first slot—but preserve the direction as a piece of edge metadata. </para>
+      <programlisting>a_replies_b    = LOAD 'a_replies_b.tsv' AS (src: chararray, dest:chararray);
+a_b_relations  = FOREACH a_replies_b GENERATE
+  ((src &lt;= dest) ? src  : dest) AS user_a,
+  ((src &lt;= dest) ? dest : src)  AS user_b,
+  ((src &lt;= dest) ? 1 : 0)       AS a_re_b:int,
+  ((src &lt;= dest) ? 0 : 1)       AS b_re_a:int;
+DUMP a_b_relations</programlisting>
+      <screen>(mrflip,tom_e_white,1,0)
+(josephkelly,wattsteve,0,1)
+(mrflip,mza,1,0)
+(nealrichter,tlipcon,0,1)</screen>
+      <para>Now gather all edges for each node pair together, and check for symmetry:</para>
+      <programlisting>a_b_relations_g   = GROUP a_b_relations BY (user_a, user_b);
+a_symmetric_b_all = FOREACH a_b_relations_g GENERATE 
+  group.user_a AS user_a, 
+  group.user_b AS user_b,
+  ( ((SUM(a_b_relations.a_re_b) > 0) AND (SUM(a_b_relations.b_re_a) > 0)) ? 1 : 0)
+    AS is_symmetric:int;
+DUMP a_symmetric_b</programlisting>
+      <screen>(mrflip,tom_e_white,1)
+(mrflip,mza,0)
+(josephkelly,wattsteve,0)
+(nealrichter,tlipcon,1)
+...</screen>
+      <programlisting>a_symmetric_b = FILTER a_symmetric_b_all BY (is_symmetric == 1);
+STORE a_symmetric_b INTO 'a_symmetric_b.tsv';</programlisting>
+      <para>Here’s a portion of the output, showing that <literal>@mrflip</literal> and <literal>@tom_e_white</literal> have a symmetric link:</para>
+      <screen>(mrflip,tom_e_white,1)
+(nealrichter,tlipcon,1)
+...</screen>
+    </sect2>
+    <sect2>
+      <title>Community Extraction</title>
+      <para>So far we’ve generated a node measure (in-degree) and an edge measure (symmetric link identification). Let’s move out one step and look at a neighborhood measure: how many of a given person’s friends are friends with each other? Along the way, we’ll produce the edge set for a visualization like the one above.</para>
+      <sect3>
+        <title>Get neighbors</title>
+        <para>Choose a seed node. First round up the seed’s neighbors:</para>
+        <programlisting>a_replies_b    = LOAD 'a_replies_b.tsv' AS (src: chararray, dest:chararray);
+-- Extract edges that originate or terminate on the seed
+n0_edges = FILTER a_replies_b BY (src == 'hadoop') OR (dest == 'hadoop');
+-- Choose the node in each pair that *isn't* our seed:
+n1_nodes_all = FOREACH n0_edges GENERATE ((src == 'hadoop') ? dest : src)
+  AS screen_name;
+n1_nodes    = DISTINCT n1_nodes_all;
+DUMP n1_nodes</programlisting>
+        <para>Now intersect the set of neighbors with the set of starting nodes to find all edges originating in <literal>n1_nodes</literal>:</para>
+        <programlisting>n1_edges_left_j    = JOIN a_replies_b BY src, n1_nodes
+  BY screen_name USING 'replicated';
+n1_edges_left      = FOREACH n1_edges_left_j GENERATE src, dest;</programlisting>
+        <para>Our copy of the graph has more than 1 billion edges, far too large to fit in memory. On the other hand, the neighbor count for a single user rarely exceeds a couple million, which fits easily in memory. Including <literal>USING 'replicated'</literal> in the JOIN command instructs Pig to do a map-side join (also called a <firstterm>fragment replicate join</firstterm>). Pig holds the <literal>n0_nodes</literal> relation in memory as a lookup table, and streams the full edge list past. Whenever the join condition is met—<literal>user_a</literal> is in the <literal>n0_nodes</literal> lookup table—it produces output. No reduce step makes for an enormous speedup!</para>
+        <para>Finally, take all the edges out from {} and retain only those ending in to {Nbrs}.</para>
+        <programlisting>-- Among those edges, find those that terminate in n1_nodes as well
+n1_edges_j = JOIN n1_edges_left BY dest, n1_nodes
+  BY screen_name USING 'replicated';
+n1_edges    = FOREACH n1_edges_j GENERATE src, dest;
+DUMP n1_edges</programlisting>
+        <screen>(mrflip,tom_e_white)
+(mrflip,mza)
+(wattsteve,josephkelly)
+(nealrichter,tlipcon)
+(bradfordcross,lusciouspear)
+(mrflip,jeromatron)
+(mndoci,mrflip)
+(nealrichter,datajunkie)</screen>
+      </sect3>
+      <sect3>
+        <title>Community metrics and the 1 billion x 1 billion problem</title>
+        <para><xref linkend="BigDataTwitterCommunity"
+            /> displays the community extracted using <literal>@hadoop</literal>, <literal
+            >@cloudera</literal> and <literal
+            >@infochimps</literal> as seeds. As you can see, the big data community is very interconnected. The link neighborhood of a celebrity such as <literal
+            >@THE_REAL_SHAQ</literal> is far more sparse.</para>
+        <figure id="BigDataTwitterCommunity">
+          <title>Big data community on Twitter</title>
+          <mediaobject>
+            <imageobject>
+              <imagedata fileref="diagrams/n1atsigns2.png" format="PNG"/>
+            </imageobject>
+          </mediaobject>
+        </figure>
+        <para>We can characterize this using the <firstterm>clustering coefficient</firstterm>: the number of <literal>n1_edges</literal> to the maximum number of possible <literal>n1_edges</literal>. It ranges from zero (no neighbor links to any other neighbor) to one (every neighbor links to every other neighbor). A high clustering coefficient indicates a cohesive community. A low clustering coefficient could indicate widely-dispersed interest (as it does with <literal>@THE_REAL_SHAQ</literal>), or it could indicate the kind of inorganic community that a spam account would engender.</para>
+        <para>Don’t try to recklessly generalize the calculation above to the full dataset. Remember the wide variation in node degree on the social graph discussed above? This skewness leads to an explosion of data—pop star <literal>@britneyspears</literal> (5.2M followers, 420k following as of July 2010) or <literal>@WholeFoods</literal> (1.7M followers, 600k following) will each generate trillions of entries. What’s worse, almost all of these will be thrown away: as just argued, the clustering coefficient for celebrities is very low! There is a very elegant way to do this on the full graph<footnote>
+            <para>See <ulink url="http://www.slideshare.net/ydn/3-xxl-graphalgohadoopsummit2010"
+                />, one of many fine tricks by Jake Hofman (<literal
+                >@jakehofman</literal>) and Sergei Vassilvitskii (<literal
+              >@vsergei</literal>) of Yahoo! Research.</para>
+          </footnote>, but, if you’re willing to assume that <literal>@britneyspears</literal> isn’t <emphasis
+            role="italic"
+          >really</emphasis> friends with 420,000 people, you can still do a reasonable job. Just weight each edge (by number of replies, whether it’s symmetric, and so on) and set limits on the number of links from any node.</para>
+        <para>
+        </para>
+        <para role="right">—Philip (flip) Kromer</para>
+      </sect3>
+    </sect2>
+  </sect1>
+</chapter>
